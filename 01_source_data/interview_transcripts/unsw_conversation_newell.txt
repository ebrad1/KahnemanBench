Interview 25. UNSW 
Here is the full transcript of the UNSW conversation with speakers Daniel Kahneman and Ben Newell.
BEN NEWELL: Welcome to Daniel Kahneman in conversation. My name is Ben Newell and I'm a Professor of Cognitive Psychology here at UNSW Sydney. This event is presented by the UNSW Centre For Ideas. Throughout the discussion you can comment on Facebook or use the live chat on YouTube or post on Twitter using the hashtag UNSWideas. I would like to acknowledge the Bedegal people that are the traditional custodians of this land. I would also like to pay my respects to the Elders both past and present and extend that respect to other Aboriginal and Torres Strait Islanders who are present here today. This event celebrates the launch of the book Noise, written by Daniel Kahneman with Oliver Sibony and Cass Sunstein. It is my very great pleasure to introduce Daniel Kahneman.
Daniel Kahneman is best known for his work with Amos Tversky on human judgment and decision making, for which he was awarded the Nobel Prize in Economics in 2002. Kahneman has also studied a number of other topics, including attention, the memory of experiences, wellbeing, counterfactual thinking and behavioural economics. He published Thinking Fast and Slow in 2010 which has sold more than 7 million copies worldwide. He is currently the Eugene Higgins Professor of Psychology at Princeton University. Today Professor Kahneman is joining us from New York.
Danny, thank you so much for speaking with me today.
DANIEL KAHNEMAN: My pleasure.
BEN NEWELL: I'd like to begin by asking a question that some of our audience might wonder about. How does a psychologist end up being awarded the Nobel Prize in Economics?
DANIEL KAHNEMAN: Well, of course, you know, luck must be involved and luck was involved. Amos Tversky and I studied judgment and decision making, which are topics that economists are interested in. Decision theory, which we were working on, is considered one of the foundations of economic theory and a paper that we published on decision making was published in the major theory journal of economics called Econometrica. We didn't publish it because we wanted to influence economics, that was not our objective, but that was just the major journal for publishing papers on decision making. But economists paid attention to the work. It influenced a few people. It influenced in particular Richard Thaler, who's now known as the guru of behavioural economics and got a Nobel Prize four years ago and is my best living friend.
And we influenced him, we published a few articles together. He created behavioural economics and I got too much credit for what he did. So that's how it happened.
BEN NEWELL: I think that's a very modest response. I wonder - so a lot of that analysis and paper and discussion came out of thinking about whether or not people are rational and one question that I always wonder about is to what extent does it make sense to describe people as rational versus irrational, given that that word has so many meanings, different meanings to different people?
DANIEL KAHNEMAN: Well, I think it makes very little sense. I very rarely use the word rational and I never use the word irrational. Rationality is a technical concept in decision theory and it's a logic. So it tells you how to - how your beliefs and your preferences must be organised to be internally consistent and coherent and this is how rationality is defined. And rationality as defined is completely impractical for human minds. A human mind, finite human mind, simply cannot meet the requirements of rationality as defined in decision theory.
So the question of whether people are rational or not is in some sense not even an interesting question. Rationality is sort of a convenient assumption for economists. And it is true that when Amos Tversky and I did our work like more than 40 years ago, economists sort of believed in rationality as a useful description of how people think. They didn't believe their wives or spouses were rational, they didn't believe their deans were rational - that was a comment that Amos Tversky always made - but they believed that people in general are rational.
and the work that we did was considered a critique of that idea, of that assumption, that people in general are rational and it's an easy target. I mean, it was very easy to show that people do not satisfy the extremely demanding axioms of rationality, the logic of rationality. So that's what I think about rationality.
BEN NEWELL: Okay. I wanted to turn now briefly to discuss your previous book, the Thinking, Fast and Slow book, which has been incredibly popular and influential. One of the central ideas - indeed, the characters really in that book -
were system 1 and system 2, so the two systems that produce our judgments and decisions, and this idea has become immensely popular. I find myself talking to doctors, lawyers, government, firefighters who all now use Kahneman's two systems in their discussions. My question is do you think that this systems dichotomy has become too literal and that what might have started as a useful characterisation has ended up potentially oversimplifying the complexities of human cognition?
DANIEL KAHNEMAN: Oh, absolutely. I mean, there is a familiar rule that psychologists are taught very early which is not to explain the behaviour of people by the behaviour of little people inside their brain - homunculi they're called - and this is a no-no when you're doing psychological theory and it's a no-no that I very deliberately violated, that is, I chose the language, I didn't invent system 1 and system 2, both the ideas and the terms existed when I started working on it, on the topic. I borrowed them and developed them. But I deliberately chose this image of agents with personalities, system 1
and system 2 and they have propensities and they have traits and they do things and they interact with each other and this is clearly an oversimplification. What I really meant to say - and I was very explicit in the book about what it did mean - is that there are two types of processes, two main types of processes, and even that is an oversimplification, but there is one process that is rapid and automatic and effortless and there is another process that is effortful and controlled and, in general, slower.
So there are those two kinds of processes and all I did was in effect say the processes of type 1, think of them as if an agent called system 1 is producing them, and similarly for type 2. It turns out, and that's the reason that I did it, that people find it very easy to think about agents. Thinking about agents is a lot simpler and easier and more compelling than thinking about categories or types. So system 1 and system 2 as systems of thinking are much easier to deal with and much easier to think about than type 1 and type 2.
However - and here I agree with you completely - people have taken me much too literally and so people believe, seem to believe, quite a few of them, that I suggested there are two actual systems in the brain and that these two systems fight it out or interact with each other and I get questions like do dogs have system 2 or do infants - whether there's system 2 in infants and things that really make very little sense because that description, as I think you were indicating, has been oversimplified. So part of the appeal of the book is that oversimplification.
Part of the appeal of the book is that it made it easy for people to think about a distinction that is real and important, which is a distinction between two fundamentally different ways I think in which thoughts and ideas come to mind. Some ideas happen to you, like 2 plus 2 and then 4 just as a thought happens to you, and other ideas you've got to produce, like 24 times 17, you'd have to work at it to figure out - I think the answer is 408 - -
BEN NEWELL: I'll believe you.
DANIEL KAHNEMAN: - - but most people would have
to figure it out. I happen to remember it. And that is work and that is really very different what happens to you when you are filling in an income tax form or when you are computing multiplications or when you're deliberately searching your memory. That is a very different state of mind than the state of mind in which you are when you're responding to 2 plus 2 or responding emotionally when somebody says your mother, that evokes an emotion, or the word vomit, that evokes an emotion. Those are immediate, they happen to you.
So that's system 1 and system 2 or type 1 and type 2 and there are costs and benefits. I think knowing about system 1 and system 2 is better than not knowing that there are those two types of thinking, but at the same time, oversimplification should be discouraged if possible.
BEN NEWELL: Yeah. I guess one of my concerns with the distinction between system 1 and system 2 - and you write a little bit about this in the new book, which we'll get to in a moment - is that there can be a kind of laziness in the use of biases
and errors and almost like an abdication of responsibility that people say, "Oh, that's my system 1, there's nothing I can do about it", and it kind of reinforces perhaps in a self-fulfilling way this notion of irrational, errorful humans that can then paint perhaps too negative a picture. That's one worry I have.
DANIEL KAHNEMAN: I think you are giving the book much too much credit. I don't think it has any influence about how people think, good or bad. I don't think it helps people think much better and I don't think that it makes people more tolerant
of their own intuitive thoughts. That's a criticism I would not accept I think. I think it is true that a lot of thinking is automatic and intuitive and not founded on logical reasoning and yet completely convincing and compelling. I think that most of the things that we believe we believe for reasons that have very little to do with logic or with reasoning. So in that sense I may be more extreme in assuming that the role of type 1 processes or the role of system 1 is larger than perhaps you do.
BEN NEWELL: Okay.
Another - it's the final question on the two systems, is the contrast between the automatic and the more deliberative. The deliberative thinking is often - we describe it as effortful and hard and, you know, the thing that's involved in doing our tax returns, but there are also instances in which we seek out that effortful thought. So I'm thinking of mental games that we like to play - cross words, sudoku. Some of your very early work on attention and thinking about attention - I wonder what's your thoughts
about that value of mental effort versus the cost of mental effort?
DANIEL KAHNEMAN: Well, it's absolutely clear that mental effort is one of the major sources of joy for many people. So the states of flow, you know, that sort of extraordinary states in which people are totally absorbed in what they're doing to the point of forgetting themselves because they are so absorbed, is a marvellous state to be in and it's a very effortful state. People are intensely concentrated and intensely focused and intentional.
Their mind is not wandering. They're working and they're enjoying the work. So that is certainly the case. It is also true that when people are not deliberately and intentionally challenging themselves, the law of least effort tends to govern. That is, if there is an easy way and a harder way of getting to a goal, we have a preference for the easier way and that is true both in the mental context and in the physical context I believe.
BEN NEWELL: Okay. I'd like to turn now to your new book, Noise. You can see that I've been - it's on the camera.
You can see that I've been reading it intently and marking various different sections. So just to begin, you write provocatively that whenever there is judgment, there is noise and more of it than you think. I'm going to delve into those different types of noise in a moment, but just to start off, can you describe what you mean by noise and how it differs from that more familiar concept of bias?
DANIEL KAHNEMAN: Well - well, noise is a complicated concept, as we'll see, because there are forms of noise, but the form
of noise that we are most interested in and that motivated the writing of this book is system noise and this is not a phenomenon within one person. This is variability across people, this is variability in a system that produces judgment and there are many such systems. So the judicial system produces sentences. The underwriting system in an insurance company produces evaluations of risk and sets premiums. The patent system grants patents to some discoveries and denies them to others. The emergency room in a hospital is a system
for producing diagnoses and treatments. And now those systems when we are considering them, they are populated by different people who fulfil the same roles. So there are judges passing sentences, different underwriters, different ER physicians. And what we would want in facing such a system is we would want them to have one voice. That is, clearly you would not want the sentence of defendants, the time that a defendant would spend in prison, to be determined by which judge happened to be responsible for the case that day.
Somebody who faces an insurance company and asks for a premium really does not want the premium to be determined by a lottery. And so system noise is a problem and system noise can be viewed as a source of errors, and here maybe I should elaborate for just a minute.
BEN NEWELL: Please.
DANIEL KAHNEMAN: The concept of noise in judgment is borrowed from measurement noise and altogether I view judgment as a species of measurement and measurement noise is when you're trying to measure the same thing, the same weight, the same length of line.
When you're trying to measure the same thing with an instrument and you do that repeatedly, you really want to get measurements as close as possible. You want variability to be as small as possible. That variability is called noise and the noise - and there is an immediate analogy between measurement noise and system noise within organisations. So that's the concept of noise. It's completely different from the concept of bias. Bias is a concept within individual psychology. That is, we think of bias as a psychological process
and we detect or identify bias sometimes in a particular error in a particular judgment, but we cannot identify noise in a particular judgment. Noise is a characteristic of a set of judgments, it's a statistical concept, and in that way it's very different from bias.
BEN NEWELL: And do you think that the fact that it is a statistical concept is one of the reasons why it's remained obscure? You write about it in the book as it being obscure in the public conscience, it's not something that's widely discussed, because it's a less tangible kind of -
•	DANIEL KAHNEMAN: You know, one of the themes that I developed in my previous book, in Thinking, Fast and Slow, was that people have a preference for thinking causally and about particular events and objects and they have a lot of difficulty thinking statistically and thinking about properties, statistical - non-causal, statistical properties of ensembles of objects and that maps very precisely on to thinking about biases and thinking about noise, that thinking about bias or about - you can see it in an individual error and bias is really causal, whereas noise
is inherently statistical. And I think for that reason bias is much easier to think about. Noise is quite difficult to think about. So it tends to go undetected and undiscussed and that was the motivation for writing that book.
BEN NEWELL: And in the book you distinguish between several different types of noise - system noise, level noise, pattern noise. I wanted to talk a little bit about the pattern noise for a while, if we may. So I understood pattern noise to have these two different aspects to it, a stable pattern noise and a transient pattern noise.
So to start with, this stable pattern noise is the kind of idiosyncrasies we have as individuals. I think at one point you talk about it as a judgment personality. And so what I wonder is how we reconcile our desire for creativity, for individual difference in opinion and in thinking with this need to eliminate the unwanted variability, the noise? So, for example, in a hiring decision we might like to have different opinions from different people. How do we deal with that balance?
DANIEL KAHNEMAN: Well, there are contexts
in which we clearly want diversity, we want diversity of opinions, and we are really not interested in uniformity. So we don't want all film reviewers to have exactly the same opinion. So there are many contexts in which diversity is desirable. We define noise as undesirable variability. That is, noise is variability where you don't want it. And in the context of hiring, for example, you really have to distinguish two different aspects of the problem. You could have several people involved in hiring a candidate and if each
of them brings a separate angle, so one of them is an expert on subject matter and the other one is a psychologist who evaluates the person's characteristics, or whatever, then they are bringing different inputs to the decision. This is very different from a situation in which you have an individual who is hiring who is interviewing a candidate and is making the decision to hire and another individual could face the same candidate and make a different decision. That is noise. It's not diversity that we want.
So we want the final decision to be the same, but we very frequently want different people to provide different inputs and when they provide different inputs, we simply don't call it noise. Noise is unwanted variability in a final integrated judgment or decision.
BEN NEWELL: So thinking of it in that way puts the onus on the organisation or the system as a who will to define the situations in which noise is desirable versus - or variability, rather, is desirable versus isn't desirable?
DANIEL KAHNEMAN: Absolutely.
And altogether - I mean, this is the orientation that I have in general, and I had it I think even in the previous book, that organisations - it's much easier to improve the thinking and the decision making of organisations than to improve the thinking and the decision making of individuals. Organisations think slowly, they have procedures, you can intervene in the procedures, you can standardise procedures, and there is a chance of improving things that really doesn't exist when you're trying to improve your own thinking.
BEN NEWELL: It's an interesting thought that changing the way an organisation thinks is easier than changing the way an individual thinks. I can think of university committees where that doesn't appear to be the case, but - -
DANIEL KAHNEMAN: Well, I mean, you know, I'm not saying that it's easy. I'm just saying that changing individuals is even harder.
BEN NEWELL: Right.
DANIEL KAHNEMAN: And achieving real change is even harder.
BEN NEWELL: So the second element of pattern noise that you talk about is the transient occasion noise and the idea here
is that there are irrelevant features of the context or the situation that nonetheless influence judgments. You give an example in the book of a judge potentially being more lenient on a Monday if their football team won on the weekend. You also discuss some of the work of my colleague Joe Forgas on how mood affects people's judgments. I wonder how concerned should we be about these irrelevant features that we're potentially unaware of influencing our judgment?
DANIEL KAHNEMAN: Well, I think the general picture is this,
that - take the example of judges passing sentences. So the first thought that comes to people's mind when they think about noise is that some judges are more severe than others, so that on average there are differences in their biases. That's one type of noise. And also judges differ in - I mean, within judge there are variations from one day to the other and we should be concerned about that. We don't want - the defendant really should not - well, the defendant's fate should not be determined by, you know,
the football events of the previous Monday or by the judge's current mood. So none of these sources of noise is really acceptable and some of them are harder to cope with than others.
BEN NEWELL: I might try to relate - so one of the thoughts I had when thinking about these occasion noise or these contextual situations that affect our behaviour whilst remaining somewhat out of our awareness, it put me in mind of some of the studies that you talked about in a great deal in the earlier book, in Thinking, Fast and Slow, where they're high-profile studies
in which people's behaviour was said to be influenced by features that they were unaware of. So I'm thinking of the social priming-type studies where perhaps, you know, I read about an old person and then I walk more slowly down the corridor, and I can see by the reaction on your face that you know where this question is headed, but there was - the inability to be able to replicate some of these standout studies led you to warn of an impending train wreck for the discipline a few years back and subsequent ripples of that - your letter
across multiple disciplines. We're seeing these patterns of replication, these attempts to see what's real in our science. And I'm fascinated to know whether or not you think we've emerged from that wreckage or avoided that wreck. What's your current thinking on these sorts of studies?
DANIEL KAHNEMAN: Oh, that's a dramatic change of topic. It is true that when I wrote Thinking, Fast and Slow I was very impressed by literature on priming and those were subtle, fascinating effects where a small change in context seemed
to have a significant effect on behaviour and I think now that I was gullible. I think that I believed in these results, although if I had looked more carefully I would have seen that the studies were individually fairly weak and with small samples, the effects were too large to be true in some sense. So yes, I became - and that really hurt me because I had put a lot of faith in it. And so I wrote a letter which, by the way, I did not intend to be published - I wrote a letter to people in the priming field
and I still believed in priming when I wrote that letter. I believed in it much more than I do now, in priming has a strong effect, and I asked them to get their act together and to replicate themselves so that people would believe them because it was clear that people were already failing to replicate priming. I think the current state of play is quite interesting that researchers in priming have never admitted that they were wrong. Other people have failed to replicate them very consistently, but the main thing that's happened is that, in part as a result
of this and in part as a result of the whole issue of replication in other sciences, not only in psychology, the science of psychology has improved enormously over the last decade. I think standards have become much higher. I think sample sizes are higher. People are much more careful about their reasoning and pre-registering their studies. And the methodological quality of psychological research, it's hard to believe how much it has changed. You know, the letter that we're talking about that was not intended
to be published was written in 2012 - -
BEN NEWELL: Mmm-hmm.
DANIEL KAHNEMAN: - - and in nine years the field has really changed completely - no thanks to my letter but because of other events that were already happening within the field and in the context of replication more generally.
BEN NEWELL: So you have a very positive outlook then on the future for the discipline? You think that it's now headed much more in the right direction?
DANIEL KAHNEMAN: I mean, I think it's remarkable how much has happened in a very short time.
I mean, it is now standard to - the kinds of problems that gave rise to unreplicable findings have really been tackled and so there was a very important element of self-deception. This was not fraud, but researchers allowed themselves degrees of freedom in interpreting the results and, you know, I know because I did it myself, I caught myself having made those errors. And today people are much stricter with themselves and they have to be public about the precautions that they take in carrying out their research.
So I think the main thing, you know, the priming scandal is a minor event relatively and the change in the methodological advance in the discipline is a major event and that's what's really happened over the past decade.
BEN NEWELL: Okay. Thank you for allowing me that segue or that brief tangent there. So I want to return now to the issues that come up in the new book, in Noise, and a central feature of your career has been on trying to understand the benefits and pitfalls of intuitive judgment, and many of us often like to rely
on what we think of as our intuition when we're making these kinds of judgments. Intuition is a term which I guess is perhaps hard to define. I like the definition that you often use, which is Herbert Simon's one, that it's nothing more and nothing less than recognition. But in the book you write that intuition should not be banned, but it should be informed, disciplined and delayed. This might seem at odds with the sort of fast and automatic way that people often claim to use intuition. So why do you think we need to delay and why is it that that internal signal
delivered by intuition is so seductive? And if I might just append a question from a current student that was submitted. So Irfan Muhammad, a current UNSW student, asks "Why sometimes when we focus and eliminate all kinds of noise we don't arrive at a solution in our minds, but when we do something else, the solution can suddenly turn up like a Eureka moment?" Is that an example of what you think of as intuition?
DANIEL KAHNEMAN: Well, that's a separate question. Let's return to it. I will probably forget it by the time I finish
answering one question, but you'll have to forgive me for that. The definition of intuition as recognition is sort of a technical definition. The way that people - the definition that captures people's attention is that intuition is knowing something without knowing how you know it and that is a subjective feeling of confidence that there is something that you know or something that you understand, although you cannot quite justify it. And quite often, there's no question about it, there are intuitions that people have which are truly marvellous and
they are very rapid in many cases. So all of us - my favourite example of that is talking to your spouse on the telephone and you can tell your spouse's mood on the telephone from the first word that you hear and this is a lot of practice and we're rarely wrong. You can tell whether he or she is happy or angry or depressed. We can tell an awful lot from one word. That's intuition and it's marvellous intuition. It is usually very - it's usually correct. Many professionals have intuitions that are marvellous.
So chess players can look at a chess board and have an immediate intuition about what is the correct move. Physicians can recognise, can make a diagnosis from across the room in some cases and very likely to be correct. So intuition can be marvellous and it's a characteristic of fast thinking that it happens quickly. However, that subjective sense of having an intuition, you can get it without justification. You can get it when actually what is happening is you are going wrong, you're following a mistaken heuristic of rule of thumb.
Some inconsequential or uninformative bit of information is leading you astray. The characteristic of our thinking, of intuitive thinking, is very high subjective confidence and confidence - and that is a fundamental fact about I think human thinking is that confidence is really imperfectly correlated with accuracy. So it's true that we're confident when our intuitive thinking is right, but we're also confident when our thinking, our rapid thinking, is wrong. So the recommendation in the current book that was explicit was
to delay intuition and if you want me to I can describe the origin of that idea, but I don't know whether this is where you want me to go.
BEN NEWELL: No, I would like to hear where that idea comes from and how we know when we should delay it, I suppose, and how to do it.
DANIEL KAHNEMAN: Well, in my mind the idea goes back a very long time ago when I was a lieutenant in the Israeli Army - and that was in the 1950s and I'm really embarrassed to say how long ago it was - and I was assigned the task of setting
up an interview system for combat recruits which was really intended to evaluate the suitability of recruits to combat units and there had been an interview - there was an interview system in existence which was the standard unstructured interview where the interviewer spoke to an individual and tried to form a general impression and eventually got a sense that he or she knew the individual, could tell how good a soldier that individual would be. And that actually had very low validity. So people had high confidence in their intuitions and
they were essentially useless and this is very common - that is, it's well known that unstructured interview produces a lot of confidence in the interviewer and very poor validity on average. Now, the system that I devised under the influence of an important psychologist, Paul Meehl, who had just published a book on that - the system I devised involved the interviewer giving six scores to the recruit on different characteristics - on punctuality, on sociability, on - I had a characteristic you wouldn't use now, masculine pride,
and there were six of them and the idea was for the interviewer to ask factual questions leading to a score on each of these six attributes. And my initial plan was that we would just take the average of these six scores and that would be a judgment of how well the recruit - you know, the best guess as to how well a recruit would do. The interviewers who were on the job and who had been using unstructured board of interviewing were furious with me and they were furious because they wanted that sense of intuition, they wanted the sense of getting
to know someone and they wanted to use their clinical ability and I remember being told by one of them, "You're turning us into robots", that is by this sort of mechanical way of doing it. So I compromised and my compromise was I told them, "Well, you do things the way I told you, you get those six scores, but once you have completed the six scores, close your eyes and make a judgment, an intuitive judgment, how good a soldier would that person be." Now, a few months later we had the results of that study
and the results were that we were doing a lot better than in the unstructured interview. Today that's not a surprise at all. But what was surprising to me then was that the final intuitive judgment, close your eyes and make a judgment, was highly valid and it added content, it added validity beyond the six traits that had been evaluated. So now the lesson I drew from that was that intuition does work, but you want to resist forming an intuition too quickly. You want to resist in the more recent terms, you want to resist fast thinking.
You want to collect the information, you want to develop a profile of the case and then you can allow yourself an intuitive judgment. And this I realised many decades later can be generalised to decision making. So when you are making a decision between different investments, you can think of options as candidates and apply the same logic to options as you would to candidates, by which I mean that any option you can characterise in terms of a set of attributes that make that option more or less desirable, you can evaluate each
of these attributes in a fact-based way and collect a series of scores and create a profile for that option and then, and only then, allow yourself to have an intuitive global evaluation of that option. So that's the idea of delaying intuition and I think there is a fair amount of support for it and certainly it's a major recommendation in my last book, so it was personally quite satisfying to go back to an idea that I had developed 60 years earlier and use it again.
BEN NEWELL: Yes. I found that that theme running through was very kind of -
I guess maybe I was getting an internal signal from the coherence or the cohesiveness of the argument there. You made a comment just now about the resistance of being turned into robots and one of the decision hygiene strategies that you talk about in the book, ways to sort of avoid noise or clean up noise, is to do with relying more on algorithms, so there's a rise of advisor systems of algorithms everywhere at the moment. One of our audience, university alumni Lenore Guildthorpe, asks "Do you believe that artificial intelligence,
relying on algorithms, will be able to match the way that humans think?" We also had a question about whether - how best should we work with algorithms, how should humans interact with these algorithms?
DANIEL KAHNEMAN: Well, you know, this is a very loaded topic and artificial intelligence is going to I think produce major problems for humanity in the next few decades. But with respect to judgment, there is a long history of research covering human judgment to rules, to various - the rules can be very simple, but the essential aspect of those rules
is that they are noise free. That is, when you apply an algorithm or a rule to the same case on different occasions, you are going to come up with exactly the same answer. That's noise free. It turns out there is so much noise in human judgment that for that reason alone rules tend to be superior in many cases to human judgment and in some cases vastly superior. Now, that's even - that was true even before artificial intelligence. It was true applying simple statistical rules and even imperfect statistical
rules just applied consistently will do better than people. Now, there is a history of trying to use - to combine rules and intuition that is providing people with the input of say artificial intelligence or some statistical analysis and one of the best-known examples of that was in chess. So in 1998 Garry Kasparov, who was then the world champion in chess, was defeated by IBM Deep Blue. That was sort of a very important moment in the competition between artificial intelligence and human judgment and Garry Kasparov, who has quite an opinion as well
as a brilliant man, he really did not like the style of the computer that defeated him. He felt that there was something mechanical, robot-like in Deep Blue and the style of Deep Blue. And his idea, which he maintained for several years, was that the optimal way to play chess would be by combining a very good chess player
